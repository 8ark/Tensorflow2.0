{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4장 다변량 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이전까지는 선형회귀(linear regression)이라고 하면 y = ax + b 의 개념으로 봤다.\n",
    ">\n",
    "$$ H(x) = w x + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이제는 '다변량' 이라는 개념으로 식을 봐야 한다. y = ax1 + bx2 + cx3 + d 이런 느낌\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 변수가 많아지면서 기존의 방식으로는 계산을 진행하기 어렵다.\n",
    "* 그래서 Matrix의 개념을 도입하는 거다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_1 x_1 + w_2 x_2 + w_3 x_3 $$\n",
    ">\n",
    "$$ = \\begin{pmatrix} w_{ 1 } & w_{ 2 } & w_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} x_{ 1 } \\\\ x_{ 2 } \\\\ x_{ 3 } \\end{pmatrix} $$\n",
    ">\n",
    "$$ = WX $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그런데 W * X와 X * W 중 뭐가 더 나을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instance는 여러 값이 나오지만, weight는 고정적이다.\n",
    "* 그래서 X * W로 보는 게 깔끔(?!)\n",
    "* Tensorflow는 H(X) = XW로 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시 도전!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 함수식과 Matrix를 직접 비교해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 | x2 | y\n",
    "---- | ---- | ----\n",
    "1  |  0  |  1\n",
    "0  |  2  |  2\n",
    "3  |  0  |  3\n",
    "0  |  4  |  4\n",
    "5  |  0  |  5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # 계산 값 동일하도록 seed값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [1, 0, 3, 0, 5]\n",
    "x2 = [0, 2, 0, 4, 0]\n",
    "y  = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0)) # uniform(균등)한 형태로 -10부터 10사이에서 랜덤한 변수 설정\n",
    "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "b  = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.Variable(0.001) # learning rate는 0.001로 할까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 1256.619385 |    -9.1905 |    -1.9179 |  -8.748963\n",
      "   50 | 287.577789 |    -3.0720 |    -0.2124 |  -6.530516\n",
      "  100 |  69.545204 |    -0.2508 |     0.7721 |  -5.404896\n",
      "  150 |  19.368086 |     1.0387 |     1.3499 |  -4.809908\n",
      "  200 |   7.329938 |     1.6190 |     1.6926 |  -4.475818\n",
      "  250 |   4.214845 |     1.8726 |     1.8963 |  -4.272120\n",
      "  300 |   3.292675 |     1.9766 |     2.0163 |  -4.134976\n",
      "  350 |   2.951777 |     2.0129 |     2.0849 |  -4.032729\n",
      "  400 |   2.781655 |     2.0190 |     2.1218 |  -3.949406\n",
      "  450 |   2.668293 |     2.0120 |     2.1388 |  -3.876784\n",
      "  500 |   2.576570 |     1.9996 |     2.1433 |  -3.810526\n",
      "  550 |   2.494502 |     1.9851 |     2.1400 |  -3.748281\n",
      "  600 |   2.417632 |     1.9701 |     2.1318 |  -3.688743\n",
      "  650 |   2.344183 |     1.9551 |     2.1204 |  -3.631163\n",
      "  700 |   2.273401 |     1.9403 |     2.1072 |  -3.575097\n",
      "  750 |   2.204935 |     1.9257 |     2.0929 |  -3.520278\n",
      "  800 |   2.138608 |     1.9115 |     2.0779 |  -3.466537\n",
      "  850 |   2.074308 |     1.8976 |     2.0626 |  -3.413769\n",
      "  900 |   2.011956 |     1.8839 |     2.0472 |  -3.361898\n",
      "  950 |   1.951482 |     1.8704 |     2.0318 |  -3.310877\n",
      " 1000 |   1.892829 |     1.8572 |     2.0164 |  -3.260669\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000+1): # 1001번의 for문을 돌린다.\n",
    "    with tf.GradientTape() as tape: # with 문 안의 함수를 tape에 기록함\n",
    "        hypothesis = W1 * x1 + W2 * x2 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b]) # cost함수 내의 W1, W2, b 의 미분값을 왼쪽으로 삽입\n",
    "    W1.assign_sub(learning_rate * W1_grad) # W1에 learning_rate * 지금 W1의 기울기를 넣기\n",
    "    W2.assign_sub(learning_rate * W2_grad) # W2에 learning_rate * 지금 W2의 기울기를 넣기\n",
    "    b.assign_sub(learning_rate * b_grad)   # b에  learning_eate * b의 기울기를 넣기\n",
    "    \n",
    "    # 한 번 결과를 볼까\n",
    "    \n",
    "    if i % 50 == 0: # 50번에 한 번씩 아래로 ㄱㄱ\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬식(Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    [1., 0., 3., 0., 5.],  # 행렬 형태로 데이터를 저장\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.uniform((1, 2), -1.0, 1.0)) # (1, 2) # 1행 2열\n",
    "b  = tf.Variable(tf.random.uniform((1, ), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.Variable(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   0.067913 |     0.8377 |     0.8072 |   0.617546\n",
      "   50 |   0.065871 |     0.8401 |     0.8102 |   0.608213\n",
      "  100 |   0.063892 |     0.8425 |     0.8131 |   0.599015\n",
      "  150 |   0.061971 |     0.8449 |     0.8159 |   0.589952\n",
      "  200 |   0.060109 |     0.8473 |     0.8187 |   0.581024\n",
      "  250 |   0.058302 |     0.8496 |     0.8215 |   0.572229\n",
      "  300 |   0.056550 |     0.8519 |     0.8242 |   0.563567\n",
      "  350 |   0.054850 |     0.8541 |     0.8269 |   0.555034\n",
      "  400 |   0.053202 |     0.8563 |     0.8295 |   0.546631\n",
      "  450 |   0.051603 |     0.8585 |     0.8321 |   0.538354\n",
      "  500 |   0.050052 |     0.8606 |     0.8346 |   0.530203\n",
      "  550 |   0.048548 |     0.8627 |     0.8371 |   0.522174\n",
      "  600 |   0.047088 |     0.8648 |     0.8396 |   0.514268\n",
      "  650 |   0.045673 |     0.8669 |     0.8420 |   0.506481\n",
      "  700 |   0.044300 |     0.8689 |     0.8444 |   0.498812\n",
      "  750 |   0.042969 |     0.8709 |     0.8468 |   0.491259\n",
      "  800 |   0.041678 |     0.8728 |     0.8491 |   0.483820\n",
      "  850 |   0.040425 |     0.8747 |     0.8514 |   0.476494\n",
      "  900 |   0.039210 |     0.8766 |     0.8536 |   0.469279\n",
      "  950 |   0.038032 |     0.8785 |     0.8559 |   0.462173\n",
      " 1000 |   0.036889 |     0.8803 |     0.8580 |   0.455175\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x) + b # 3장에서도 언급했듯, matmul은 행렬곱이다.\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "        \n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1],b.numpy()[0]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
