{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4장 다변량 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이전까지는 선형회귀(linear regression)이라고 하면 y = ax + b 의 개념으로 봤다.\n",
    ">\n",
    "$$ H(x) = w x + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이제는 '다변량' 이라는 개념으로 식을 봐야 한다. y = ax1 + bx2 + cx3 + d 이런 느낌\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 변수가 많아지면서 기존의 방식으로는 계산을 진행하기 어렵다.\n",
    "* 그래서 Matrix의 개념을 도입하는 거다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_1 x_1 + w_2 x_2 + w_3 x_3 $$\n",
    ">\n",
    "$$ = \\begin{pmatrix} w_{ 1 } & w_{ 2 } & w_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} x_{ 1 } \\\\ x_{ 2 } \\\\ x_{ 3 } \\end{pmatrix} $$\n",
    ">\n",
    "$$ = WX $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그런데 W * X와 X * W 중 뭐가 더 나을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instance는 여러 값이 나오지만, weight는 고정적이다.\n",
    "* 그래서 X * W로 보는 게 깔끔(?!)\n",
    "* Tensorflow는 H(X) = XW로 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시 도전!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 함수식과 Matrix를 직접 비교해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 | x2 | y\n",
    "---- | ---- | ----\n",
    "1  |  0  |  1\n",
    "0  |  2  |  2\n",
    "3  |  0  |  3\n",
    "0  |  4  |  4\n",
    "5  |  0  |  5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # 계산 값 동일하도록 seed값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [1, 0, 3, 0, 5]\n",
    "x2 = [0, 2, 0, 4, 0]\n",
    "y  = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0)) # uniform(균등)한 형태로 -10부터 10사이에서 랜덤한 변수 설정\n",
    "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "b  = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.Variable(0.001) # learning rate는 0.001로 할까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 1256.619385 |    -9.1905 |    -1.9179 |  -8.748963\n",
      "   50 | 287.577789 |    -3.0720 |    -0.2124 |  -6.530516\n",
      "  100 |  69.545204 |    -0.2508 |     0.7721 |  -5.404896\n",
      "  150 |  19.368086 |     1.0387 |     1.3499 |  -4.809908\n",
      "  200 |   7.329938 |     1.6190 |     1.6926 |  -4.475818\n",
      "  250 |   4.214845 |     1.8726 |     1.8963 |  -4.272120\n",
      "  300 |   3.292675 |     1.9766 |     2.0163 |  -4.134976\n",
      "  350 |   2.951777 |     2.0129 |     2.0849 |  -4.032729\n",
      "  400 |   2.781655 |     2.0190 |     2.1218 |  -3.949406\n",
      "  450 |   2.668293 |     2.0120 |     2.1388 |  -3.876784\n",
      "  500 |   2.576570 |     1.9996 |     2.1433 |  -3.810526\n",
      "  550 |   2.494502 |     1.9851 |     2.1400 |  -3.748281\n",
      "  600 |   2.417632 |     1.9701 |     2.1318 |  -3.688743\n",
      "  650 |   2.344183 |     1.9551 |     2.1204 |  -3.631163\n",
      "  700 |   2.273401 |     1.9403 |     2.1072 |  -3.575097\n",
      "  750 |   2.204935 |     1.9257 |     2.0929 |  -3.520278\n",
      "  800 |   2.138608 |     1.9115 |     2.0779 |  -3.466537\n",
      "  850 |   2.074308 |     1.8976 |     2.0626 |  -3.413769\n",
      "  900 |   2.011956 |     1.8839 |     2.0472 |  -3.361898\n",
      "  950 |   1.951482 |     1.8704 |     2.0318 |  -3.310877\n",
      " 1000 |   1.892829 |     1.8572 |     2.0164 |  -3.260669\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000+1): # 1001번의 for문을 돌린다.\n",
    "    with tf.GradientTape() as tape: # with 문 안의 함수를 tape에 기록함\n",
    "        hypothesis = W1 * x1 + W2 * x2 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b]) # cost함수 내의 W1, W2, b 의 미분값을 왼쪽으로 삽입\n",
    "    W1.assign_sub(learning_rate * W1_grad) # W1에 learning_rate * 지금 W1의 기울기를 넣기\n",
    "    W2.assign_sub(learning_rate * W2_grad) # W2에 learning_rate * 지금 W2의 기울기를 넣기\n",
    "    b.assign_sub(learning_rate * b_grad)   # b에  learning_eate * b의 기울기를 넣기\n",
    "    \n",
    "    # 한 번 결과를 볼까\n",
    "    \n",
    "    if i % 50 == 0: # 50번에 한 번씩 아래로 ㄱㄱ\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬식(Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    [1., 0., 3., 0., 5.],  # 행렬 형태로 데이터를 저장\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.uniform((1, 2), -1.0, 1.0)) # (1, 2) # 1행 2열\n",
    "b  = tf.Variable(tf.random.uniform((1, ), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.Variable(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   0.067913 |     0.8377 |     0.8072 |   0.617546\n",
      "   50 |   0.065871 |     0.8401 |     0.8102 |   0.608213\n",
      "  100 |   0.063892 |     0.8425 |     0.8131 |   0.599015\n",
      "  150 |   0.061971 |     0.8449 |     0.8159 |   0.589952\n",
      "  200 |   0.060109 |     0.8473 |     0.8187 |   0.581024\n",
      "  250 |   0.058302 |     0.8496 |     0.8215 |   0.572229\n",
      "  300 |   0.056550 |     0.8519 |     0.8242 |   0.563567\n",
      "  350 |   0.054850 |     0.8541 |     0.8269 |   0.555034\n",
      "  400 |   0.053202 |     0.8563 |     0.8295 |   0.546631\n",
      "  450 |   0.051603 |     0.8585 |     0.8321 |   0.538354\n",
      "  500 |   0.050052 |     0.8606 |     0.8346 |   0.530203\n",
      "  550 |   0.048548 |     0.8627 |     0.8371 |   0.522174\n",
      "  600 |   0.047088 |     0.8648 |     0.8396 |   0.514268\n",
      "  650 |   0.045673 |     0.8669 |     0.8420 |   0.506481\n",
      "  700 |   0.044300 |     0.8689 |     0.8444 |   0.498812\n",
      "  750 |   0.042969 |     0.8709 |     0.8468 |   0.491259\n",
      "  800 |   0.041678 |     0.8728 |     0.8491 |   0.483820\n",
      "  850 |   0.040425 |     0.8747 |     0.8514 |   0.476494\n",
      "  900 |   0.039210 |     0.8766 |     0.8536 |   0.469279\n",
      "  950 |   0.038032 |     0.8785 |     0.8559 |   0.462173\n",
      " 1000 |   0.036889 |     0.8803 |     0.8580 |   0.455175\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x) + b # 3장에서도 언급했듯, matmul은 행렬곱이다.\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "        \n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1],b.numpy()[0]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis 에 bias를 넣는다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "    [1., 1., 1., 1., 1.],\n",
    "    [1., 0., 3., 0., 5.],\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  24.178761 |     0.7083 |    -0.9680 |     0.2746\n",
      "   50 |   5.537626 |     0.9666 |    -0.0852 |     0.4291\n",
      "  100 |   1.427081 |     1.0807 |     0.3289 |     0.5153\n",
      "  150 |   0.512747 |     1.1263 |     0.5241 |     0.5657\n",
      "  200 |   0.303499 |     1.1395 |     0.6171 |     0.5967\n",
      "  250 |   0.250722 |     1.1371 |     0.6626 |     0.6171\n",
      "  300 |   0.233131 |     1.1273 |     0.6860 |     0.6314\n",
      "  350 |   0.223717 |     1.1141 |     0.6991 |     0.6421\n",
      "  400 |   0.216380 |     1.0992 |     0.7075 |     0.6507\n",
      "  450 |   0.209707 |     1.0836 |     0.7136 |     0.6580\n",
      "  500 |   0.203352 |     1.0678 |     0.7186 |     0.6644\n",
      "  550 |   0.197222 |     1.0519 |     0.7232 |     0.6703\n",
      "  600 |   0.191288 |     1.0362 |     0.7275 |     0.6758\n",
      "  650 |   0.185536 |     1.0206 |     0.7317 |     0.6810\n",
      "  700 |   0.179959 |     1.0052 |     0.7358 |     0.6860\n",
      "  750 |   0.174550 |     0.9901 |     0.7397 |     0.6909\n",
      "  800 |   0.169304 |     0.9751 |     0.7437 |     0.6957\n",
      "  850 |   0.164215 |     0.9603 |     0.7476 |     0.7004\n",
      "  900 |   0.159280 |     0.9458 |     0.7514 |     0.7049\n",
      "  950 |   0.154493 |     0.9315 |     0.7551 |     0.7094\n",
      " 1000 |   0.149849 |     0.9174 |     0.7588 |     0.7138\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random.uniform((1, 3), -1.0, 1.0)) # [1, 3]의 틀을 만들고 bias를 없앰\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate) # 갑분 케라스...?\n",
    "    # SGD는 확률적 경사하강법으로 keras로 정말 간단하게 쓸 수 있음\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, X) # W와 X를 Matrix 곱 / 여기서는 b를 신경 안 씀\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "    \n",
    "    # W의 기울기를 구해보면?\n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads,[W]))\n",
    "    \n",
    "    if i % 50 == 0: # 50번씩 마다 아래로 내려감\n",
    "        print(\"{:5} | {:10.6f} | {: 10.4f} | {: 10.4f} | {:10.4f}\".format(\n",
    "        i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction exam score (시험 성적 맞춰보기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 (quiz 1) | x2 (quiz 2) | x3 (mid 1) | Y (final)\n",
    "---- | ---- | ----| ----\n",
    "73 | 80 | 75 | 152\n",
    "93 | 88 | 93 | 185\n",
    "89 | 91 | 90 | 180\n",
    "96 | 98 | 100 | 196\n",
    "73 | 66 | 70 | 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0) # 값 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X값, 그리고 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [73., 93., 89., 96., 73.]\n",
    "x2 = [80., 88., 91., 98., 66.]\n",
    "x3 = [75., 93., 90., 100., 70.]\n",
    "y = [152., 185., 180., 196., 142.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weight 초기값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(10.)\n",
    "w2 = tf.Variable(10.)\n",
    "w3 = tf.Variable(10.)\n",
    "b  = tf.Variable(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 식 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 1816078.3750\n",
      "   50 |       1.9075\n",
      "  100 |       1.8760\n",
      "  150 |       1.8453\n",
      "  200 |       1.8155\n",
      "  250 |       1.7864\n",
      "  300 |       1.7580\n",
      "  350 |       1.7304\n",
      "  400 |       1.7034\n",
      "  450 |       1.6772\n",
      "  500 |       1.6516\n",
      "  550 |       1.6266\n",
      "  600 |       1.6023\n",
      "  650 |       1.5787\n",
      "  700 |       1.5556\n",
      "  750 |       1.5331\n",
      "  800 |       1.5111\n",
      "  850 |       1.4898\n",
      "  900 |       1.4689\n",
      "  950 |       1.4486\n",
      " 1000 |       1.4288\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00001\n",
    "\n",
    "for i in range(1000+1):\n",
    "    #GradientDescent를 구하기 위해서 GradientTape함수를 쓸 거다\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "    \n",
    "    # cost 함수 안의 w1, w2, w3, b 값들의 미분값을 왼쪽에 삽입\n",
    "    W1_grad, W2_grad, W3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    #for문마다 각 변수값을 업데이트 해준다.\n",
    "    \n",
    "    w1.assign_sub(learning_rate * W1_grad)\n",
    "    w2.assign_sub(learning_rate * W2_grad)\n",
    "    w3.assign_sub(learning_rate * W3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변량 회귀 분석 (방법 1, 무식 버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 집어넣기\n",
    "x1 = [73., 93., 89., 96., 73.]\n",
    "x2 = [80., 88., 91., 98., 66.]\n",
    "x3 = [75., 93., 90., 100., 70.]\n",
    "Y = [152., 185., 180., 196., 142.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight값 랜덤하게 넣기\n",
    "w1 = tf.Variable(tf.random.normal((1,)))\n",
    "w2 = tf.Variable(tf.random.normal((1,)))\n",
    "w3 = tf.Variable(tf.random.normal((1,)))\n",
    "b  = tf.Variable(tf.random.normal((1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate 설정\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   54173.1680\n",
      "   50 |      13.0500\n",
      "  100 |      12.7070\n",
      "  150 |      12.3732\n",
      "  200 |      12.0483\n",
      "  250 |      11.7321\n",
      "  300 |      11.4243\n",
      "  350 |      11.1248\n",
      "  400 |      10.8333\n",
      "  450 |      10.5495\n",
      "  500 |      10.2734\n",
      "  550 |      10.0046\n",
      "  600 |       9.7430\n",
      "  650 |       9.4884\n",
      "  700 |       9.2406\n",
      "  750 |       8.9995\n",
      "  800 |       8.7648\n",
      "  850 |       8.5363\n",
      "  900 |       8.3140\n",
      "  950 |       8.0976\n",
      " 1000 |       7.8870\n"
     ]
    }
   ],
   "source": [
    "# 식 구성\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 + w2 *x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "        # hypothesis와 cost 함수를 tape 안에 집어넣는다.\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # w1, w2, w3, b의 기울기를 통해 각 값들을 변경하자\n",
    "    \n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    # 그리고 for문이 50번씩 돌때마다 아래로 내려가서 print\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변량 회귀분석 (행렬 기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬 기반으로 데이터 삽입\n",
    "# 데이터 정리상태부터 다름\n",
    "\n",
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 슬라이싱\n",
    "X = data[:, :-1]\n",
    "Y = data[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤하게 W값 설정\n",
    "W = tf.Variable(tf.random.normal((3,1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate 설정\n",
    "learning_rate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 | 71088.7344\n",
      "   50 |   796.1061\n",
      "  100 |    16.1204\n",
      "  150 |     7.4498\n",
      "  200 |     7.3378\n",
      "  250 |     7.3208\n",
      "  300 |     7.3049\n",
      "  350 |     7.2890\n",
      "  400 |     7.2732\n",
      "  450 |     7.2575\n",
      "  500 |     7.2418\n",
      "  550 |     7.2261\n",
      "  600 |     7.2105\n",
      "  650 |     7.1949\n",
      "  700 |     7.1793\n",
      "  750 |     7.1638\n",
      "  800 |     7.1484\n",
      "  850 |     7.1329\n",
      "  900 |     7.1176\n",
      "  950 |     7.1022\n",
      " 1000 |     7.0869\n",
      " 1050 |     7.0717\n",
      " 1100 |     7.0564\n",
      " 1150 |     7.0412\n",
      " 1200 |     7.0261\n",
      " 1250 |     7.0110\n",
      " 1300 |     6.9959\n",
      " 1350 |     6.9809\n",
      " 1400 |     6.9659\n",
      " 1450 |     6.9510\n",
      " 1500 |     6.9360\n",
      " 1550 |     6.9212\n",
      " 1600 |     6.9064\n",
      " 1650 |     6.8916\n",
      " 1700 |     6.8769\n",
      " 1750 |     6.8622\n",
      " 1800 |     6.8475\n",
      " 1850 |     6.8328\n",
      " 1900 |     6.8182\n",
      " 1950 |     6.8037\n",
      " 2000 |     6.7892\n"
     ]
    }
   ],
   "source": [
    "# hypothesis, prediction 함수 구성\n",
    "\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b # 이게 지금 hypothesis\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs + 1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean(tf.square(predict(X) - Y)) # cost 값 구현\n",
    "        \n",
    "    # 기울기 구하기\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    # 새롭게 구한 기울기로 각 변수 값 재설정\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05773982],\n",
       "       [0.6998049 ],\n",
       "       [1.2404082 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W1, W2, W3 각 weight값 구하기\n",
    "W.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5436951], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bias 값 구하기\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=145259, shape=(5, 1), dtype=float32, numpy=\n",
       "array([[153.77371],\n",
       "       [182.85431],\n",
       "       [181.00153],\n",
       "       [198.70842],\n",
       "       [137.77441]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변수와 bias를 더한 Y값은???\n",
    "tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[152.],\n",
       "       [185.],\n",
       "       [180.],\n",
       "       [196.],\n",
       "       [142.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y # labels, 실제값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[153.77371],\n",
       "       [182.85431],\n",
       "       [181.00153],\n",
       "       [198.70842],\n",
       "       [137.77441]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction, 예측값이다.\n",
    "predict(X).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[186.28157],\n",
       "       [175.21059]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 통해서 새로운 데이터를 넣었을 때 예측을 해본다\n",
    "\n",
    "predict([[89., 95., 92.], [84., 92., 85.]]).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
