{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6장 소프트맥스 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(100) # 반복실행 시 값 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression이 이진분류(Binary Classfication)이라면\n",
    "* Softmax Regression은 다진분류(Multinomial Classification)의 과정\n",
    "* A or not, B or not, C or not의 방식으로 Hypothesis를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/RV5Hbqsb/image1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 매번 식을 통해서 하기보다 Matrix 형태를 통해 다진 분류의 과정을 진행한다.\n",
    "* 그리고 이렇게 나온 Hypothesis를 logistic regression에 넣어 A or not, B or not, C or not의 과정으로 이진 분류를 각각 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/L62s1P0R/image2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그런데 이렇게 나온 score를 단순히 쓰는 게 아니라 '어떠한' 과정을 통해 해당 값의 확률로 전환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/RCQS4FCY/image3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* '어떠한' 방법이란 S(y), 즉 '소프트맥스 함수'\n",
    "* 1. 각 값을 0 ~ 1 사이의 값으로\n",
    "* 2. 전체를 더하면 1의 상태로 파악\n",
    "* 3. 즉 결과값을 '확률'로 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/jjPVcgL1/image6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. 그렇게 나온 확률에 기반한 Y값은\n",
    "* 2. 원-핫 인코딩을 통해 그 중 하나의 값만 정하고 나머지를 없애도록 한다. (argmax 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/V6Pwcyx1/image4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. 그렇게 확보한 Softmax값과 실제값을 '크로스 엔트로피(Cross Entropy)'의 과정을 통해 Cost Function을 구함\n",
    "* 2. 앗...아니 왜 기껏 구한 Si값에 log를 씌우냐고?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/9FQh2Yn4/image5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. -log 는 (x = 0 에서 infinite), (x = 1에서 0) 이야\n",
    "* 2. 실제값이 0 일 때, 예측값이 0일 경우에는 -log값이 무한이어도 (곱했을 때 결과가 0) 이지만\n",
    "* 3. 실제값이 1일 때, 예측값이 0이면 -log값이 무한이면 (곱했을 때 결과도 무한)으로 나타남.\n",
    "* 4. 즉, 실제값과 다를 경우, 미친 듯이 값이 error가 치솟는 거죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/43CqY6BQ/image7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. 그렇게 구한 error값들을 모아 평균을 내면 우리가 평소에 구했던 Cost function 이 나옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/0j3ZLv40/image8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. 그리고 그렇게 구한 Cost funtion, 즉 loss값을 미분하면\n",
    "* 2. Gradient Descent값을 구할 수 있고\n",
    "* 3. 그렇게 구한 Gradient Descent값으로 weight값을 수정합니다.\n",
    "* 4. 그리고 개선된 weight값으로 다시 Cost를 구하는 것\n",
    "* 5. 그게 우리가 아는 함수의 진행 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/rw5gYvN5/image9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실제 값을 가지고 진행해볼까요?\n",
    "* 우리는 세 집단의 Y값을 가지고 분류 과정을 진행하네요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [\n",
    "    [1, 2, 1, 1],\n",
    "    [2, 1, 3, 2],\n",
    "    [3, 1, 3, 4],\n",
    "    [4, 1, 5, 5],\n",
    "    [1, 7, 5, 5],\n",
    "    [1, 2, 5, 6],\n",
    "    [1, 6, 6, 6],\n",
    "    [1, 7, 7, 7]\n",
    "]\n",
    "\n",
    "y_data = [\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 1, 1],\n",
       " [2, 1, 3, 2],\n",
       " [3, 1, 3, 4],\n",
       " [4, 1, 5, 5],\n",
       " [1, 7, 5, 5],\n",
       " [1, 2, 5, 6],\n",
       " [1, 6, 6, 6],\n",
       " [1, 7, 7, 7]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X값은 4차원 형태고, Y값은 3종류로 분류됨\n",
    "* 위 데이터 값은 지금 int(정수)일 테니, float(실수)로 한 번에 바꿔주자\n",
    "* 이렇게 하면 예전처럼 숫자에 매번 .을 붙이지 않아도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.asarray(x_data, dtype = np.float32)\n",
    "y_data = np.asarray(y_data, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 1., 1.],\n",
       "       [2., 1., 3., 2.],\n",
       "       [3., 1., 3., 4.],\n",
       "       [4., 1., 5., 5.],\n",
       "       [1., 7., 5., 5.],\n",
       "       [1., 2., 5., 6.],\n",
       "       [1., 6., 6., 6.],\n",
       "       [1., 7., 7., 7.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 3 # 클래스의 개수 설정 // 3개로 분류한다는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight와 bias를 셋팅하자\n",
    "W = tf.Variable(tf.random.normal((4, nb_classes)), name = 'weight') # 열 위치는 각 column의 정렬이기에 nb_classes 넣기\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name = 'bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* W와 b를 variable이라는 그릇에 담기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [W, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[ 0.16052227, -1.6597689 , -1.232133  ],\n",
      "       [ 0.5971658 ,  1.0609883 , -1.3277572 ],\n",
      "       [-0.27911443, -0.02141875, -1.502249  ],\n",
      "       [ 0.3066489 ,  0.5355358 , -1.3167298 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([ 0.64973587,  0.32791495, -0.7519815 ], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hypothesis 함수 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500\" src=\"https://t1.daumcdn.net/cfile/tistory/9959FF345AAD52DF03\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. X * W 를 통해 score를 구함 = (2.0, 1.0, 0.1)\n",
    "* 2. softmax를 통과시켜 전체 비율 중 해당 결과값이 확률(0.7, 0.2, 0.1)을 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.7430651e-01 3.2564253e-01 5.0943025e-05]\n",
      " [9.0616632e-01 9.3831733e-02 1.9914553e-06]\n",
      " [9.7417474e-01 2.5825273e-02 2.0688438e-08]\n",
      " [9.9104238e-01 8.9576421e-03 8.9317338e-11]\n",
      " [2.8262276e-02 9.7173780e-01 1.6018376e-15]\n",
      " [1.9041234e-01 8.0958766e-01 3.2210564e-11]\n",
      " [2.7643861e-02 9.7235614e-01 6.2340206e-16]\n",
      " [1.0871028e-02 9.8912901e-01 2.0760157e-18]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax에 대한 원핫인코딩을 테스트해보자\n",
    "\n",
    "sample_db = [\n",
    "    [8, 2, 1, 4]\n",
    "]\n",
    "sample_db = np.asarray(sample_db, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[9.9999714e-01 2.8073625e-06 3.3837280e-11]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 보면 '9.9999714e-01' 는 9.999 * 10^-1 이니까 0.999 . 사실상 1이다. \n",
    "print(hypothesis(sample_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/9FQh2Yn4/image5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/DyXc8tDR/ch6-2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.721287, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_mean(tf.reduce_sum(Y * tf.math.log(logits), axis =1)) # axis = 차원의 종류의 개수    \n",
    "# 예시 코드는 cost, cost_mean으로 나누면서 tf.reduce_mean을 붙이는 거를 나눴는데\n",
    "# 아마 한 번에 이해하기 힘들 수 있어서 그렇게 나눈 듯\n",
    "    return cost\n",
    "\n",
    "print(cost_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width  = 100% src = \"https://i.postimg.cc/rw5gYvN5/image9.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables) # loss 함수를 variables로 미분한 값\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=110, shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 0.95381534, -0.2038221 , -0.74999315],\n",
      "       [-0.9949229 ,  1.49491   , -0.49998704],\n",
      "       [-0.04926568,  0.9242586 , -0.87499285],\n",
      "       [-0.01696306,  0.8919562 , -0.87499315]], dtype=float32)>, <tf.Tensor: id=108, shape=(3,), dtype=float32, numpy=array([ 0.22535992,  0.14963347, -0.37499338], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(grad_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* optimizer를 통해 최적의 Cost값을 찾자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, epochs = 2000, verbose = 100):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1) #SGD = 확률적 경사하강법\n",
    "    \n",
    "    for i in range(epochs): # 2000번의 epochs를 돌린다.\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i == 0) | ((i +1) % verbose ==0): #만약 i가 0, 혹은 verbose로 나눴을때 0이면\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여기에 우리의 예시 값들을 넣어주면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 5.983233\n",
      "Loss at epoch 100: 0.694579\n",
      "Loss at epoch 200: 0.593458\n",
      "Loss at epoch 300: 0.533314\n",
      "Loss at epoch 400: 0.486159\n",
      "Loss at epoch 500: 0.445092\n",
      "Loss at epoch 600: 0.407329\n",
      "Loss at epoch 700: 0.371230\n",
      "Loss at epoch 800: 0.335553\n",
      "Loss at epoch 900: 0.299358\n",
      "Loss at epoch 1000: 0.263271\n",
      "Loss at epoch 1100: 0.239523\n",
      "Loss at epoch 1200: 0.227540\n",
      "Loss at epoch 1300: 0.216757\n",
      "Loss at epoch 1400: 0.206913\n",
      "Loss at epoch 1500: 0.197890\n",
      "Loss at epoch 1600: 0.189591\n",
      "Loss at epoch 1700: 0.181932\n",
      "Loss at epoch 1800: 0.174844\n",
      "Loss at epoch 1900: 0.168267\n",
      "Loss at epoch 2000: 0.162148\n"
     ]
    }
   ],
   "source": [
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sample data를 넣어서 지금의 모델을 체크해보자\n",
    "* sample_data = [2, 1, 3, 2] 넣어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [[2, 1, 3, 2]] # 이 값은 우리가 x_data에서 가지고 있던 2번째 값, label 값은 [0, 0, 1] 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = np.asarray(sample_data, dtype = np.float32) # 실수로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hypothesis(sample_data) # sample_data를 넣은 hypothesis, 즉 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00149287 0.08491352 0.9135936 ]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(tf.argmax(a, 1)) # index: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class로 정리하자면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 7.850834\n",
      "Loss at epoch 500: 0.523784\n",
      "Loss at epoch 1000: 0.296680\n",
      "Loss at epoch 1500: 0.208247\n",
      "Loss at epoch 2000: 0.169967\n"
     ]
    }
   ],
   "source": [
    "class softmax_classifer(tf.keras.Model):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(softmax_classifer, self).__init__()\n",
    "        self.W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
    "        self.b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "        \n",
    "    def softmax_regression(self, X):\n",
    "        return tf.nn.softmax(tf.matmul(X, self.W) + self.b)\n",
    "    \n",
    "    def cost_fn(self, X, Y):\n",
    "        logits = self.softmax_regression(X)\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(logits), axis=1))        \n",
    "        return cost\n",
    "    \n",
    "    def grad_fn(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = self.cost_fn(x_data, y_data)\n",
    "            grads = tape.gradient(cost, self.variables)            \n",
    "            return grads\n",
    "    \n",
    "    def fit(self, X, Y, epochs=2000, verbose=500):\n",
    "        optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            grads = self.grad_fn(X, Y)\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "            if (i==0) | ((i+1)%verbose==0):\n",
    "                print('Loss at epoch %d: %f' %(i+1, self.cost_fn(X, Y).numpy()))\n",
    "            \n",
    "model = softmax_classifer(nb_classes)\n",
    "model.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
