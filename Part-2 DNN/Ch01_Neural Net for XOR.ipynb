{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch01_XOR을 위한 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용할 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 시드값 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용할 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2294b190a20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\n",
    "plt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 축에 이름 새기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\n",
    "plt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터셋 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 값을 실수로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Logistic Regression 활용해 XOR 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![1.png](https://i.postimg.cc/hjwWVt3r/1.png)](https://postimg.cc/gXvTWmpX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weight, bias 초기값 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros((2, 1)), name = 'weight')\n",
    "b = tf.Variable(tf.zeros((1, )), name = 'bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sigmoid 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features):\n",
    "    hypothesis = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cost 함수 설정\n",
    "$$\n",
    "\\begin{align}\n",
    "cost(h(x),y) & = −y log(h(x))−(1−y)log(1−h(x))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(hypothesis, features, labels):\n",
    "    cost = tf.reduce_mean(-labels *tf.math.log(hypothesis) - (1- labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimizer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sigmoid를 통해 0.5를 기준으로 y값을 0, 1 중 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype = tf.float32)) # predicted와 labels이 동일하면 1, 아니면 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GradientTape으로 경사값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features), features, labels)\n",
    "    return tape.gradient(loss_value, [W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Session 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6931\n",
      "Iter: 100, Loss: 0.6931\n",
      "Iter: 200, Loss: 0.6931\n",
      "Iter: 300, Loss: 0.6931\n",
      "Iter: 400, Loss: 0.6931\n",
      "Iter: 500, Loss: 0.6931\n",
      "Iter: 600, Loss: 0.6931\n",
      "Iter: 700, Loss: 0.6931\n",
      "Iter: 800, Loss: 0.6931\n",
      "Iter: 900, Loss: 0.6931\n",
      "Iter: 1000, Loss: 0.6931\n",
      "W = [[0.]\n",
      " [0.]], B = [0.]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels in dataset:\n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad(logistic_regression(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars = zip(grads, [W, b]))\n",
    "        \n",
    "        if step % 100 ==0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(logistic_regression(x_data),y_data)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Neural Net 활용해 XOR 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![2.png](https://i.postimg.cc/fWGkNzTm/2.png)](https://postimg.cc/K45m7yT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.normal((2, 1)), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal((1,)), name='bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal((2, 1)), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal((1,)), name='bias2')\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal((2, 1)), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal((1,)), name='bias3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(features):\n",
    "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
    "    layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n",
    "    layer3 = tf.concat([layer1, layer2],-1)\n",
    "    layer3 = tf.reshape(layer3, shape = [-1,2])\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer3, W3) + b3)\n",
    "    return hypothesis\n",
    "\n",
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(neural_net(features),labels)\n",
    "    return tape.gradient(loss_value, [W1, W2, W3, b1, b2, b3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.8487\n",
      "Iter: 5000, Loss: 0.6847\n",
      "Iter: 10000, Loss: 0.6610\n",
      "Iter: 15000, Loss: 0.6154\n",
      "Iter: 20000, Loss: 0.5722\n",
      "Iter: 25000, Loss: 0.5433\n",
      "Iter: 30000, Loss: 0.5211\n",
      "Iter: 35000, Loss: 0.4911\n",
      "Iter: 40000, Loss: 0.4416\n",
      "Iter: 45000, Loss: 0.3313\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50000\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in dataset:\n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad(neural_net(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, b1, b2, b3]))\n",
    "        if step % 5000 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(neural_net(features),labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(neural_net(x_data),y_data)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 모델링(Class 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wide_deep_nn():\n",
    "    def __init__(self, nb_classes):\n",
    "        super(wide_deep_nn, self).__init__()        \n",
    "     \n",
    "        self.W1 = tf.Variable(tf.random.normal((2, nb_classes)), name='weight1')\n",
    "        self.b1 = tf.Variable(tf.random.normal((nb_classes,)), name='bias1')\n",
    "\n",
    "        self.W2 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight2')\n",
    "        self.b2 = tf.Variable(tf.random.normal((nb_classes,)), name='bias2')\n",
    "\n",
    "        self.W3 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight3')\n",
    "        self.b3 = tf.Variable(tf.random.normal((nb_classes,)), name='bias3')\n",
    "\n",
    "        self.W4 = tf.Variable(tf.random.normal((nb_classes, 1)), name='weight4')\n",
    "        self.b4 = tf.Variable(tf.random.normal((1,)), name='bias4')\n",
    "        \n",
    "        self.variables = [self.W1,self.b1,self.W2,self.b2,self.W3,self.b3,self.W4,self.b4]\n",
    "        \n",
    "    def preprocess_data(self, features, labels):\n",
    "        features = tf.cast(features, tf.float32)\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        return features, labels\n",
    "        \n",
    "    def deep_nn(self, features):\n",
    "        layer1 = tf.sigmoid(tf.matmul(features, self.W1) + self.b1)\n",
    "        layer2 = tf.sigmoid(tf.matmul(layer1, self.W2) + self.b2)\n",
    "        layer3 = tf.sigmoid(tf.matmul(layer2, self.W3) + self.b3)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer3, self.W4) + self.b4)\n",
    "        return hypothesis\n",
    "    \n",
    "    def loss_fn(self, hypothesis, features, labels):\n",
    "        cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "        return cost\n",
    "\n",
    "    def accuracy_fn(self, hypothesis, labels):\n",
    "        predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    def grad(self, hypothesis, features, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.loss_fn(self.deep_nn(features),features,labels)\n",
    "        return tape.gradient(loss_value,self.variables)\n",
    "    \n",
    "    def fit(self, dataset, EPOCHS=20000, verbose=500):\n",
    "        optimizer =  tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "        for step in range(EPOCHS):\n",
    "            for features, labels  in dataset:\n",
    "                features, labels = self.preprocess_data(features, labels)\n",
    "                grads = self.grad(self.deep_nn(features), features, labels)\n",
    "                optimizer.apply_gradients(grads_and_vars=zip(grads, self.variables))\n",
    "                if step % verbose == 0:\n",
    "                    print(\"Iter: {}, Loss: {:.4f}\".format(step, self.loss_fn(self.deep_nn(features),features,labels)))\n",
    "                    \n",
    "    def test_model(self,x_data, y_data):\n",
    "        x_data, y_data = self.preprocess_data(x_data, y_data)\n",
    "        test_acc = self.accuracy_fn(self.deep_nn(x_data),y_data)\n",
    "        print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DNN을 통한 XOR 문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wide_deep_nn(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 1.0398\n",
      "Iter: 500, Loss: 0.6936\n",
      "Iter: 1000, Loss: 0.6923\n",
      "Iter: 1500, Loss: 0.6912\n",
      "Iter: 2000, Loss: 0.6901\n",
      "Iter: 2500, Loss: 0.6890\n",
      "Iter: 3000, Loss: 0.6879\n",
      "Iter: 3500, Loss: 0.6867\n",
      "Iter: 4000, Loss: 0.6855\n",
      "Iter: 4500, Loss: 0.6842\n",
      "Iter: 5000, Loss: 0.6827\n",
      "Iter: 5500, Loss: 0.6811\n",
      "Iter: 6000, Loss: 0.6793\n",
      "Iter: 6500, Loss: 0.6772\n",
      "Iter: 7000, Loss: 0.6749\n",
      "Iter: 7500, Loss: 0.6721\n",
      "Iter: 8000, Loss: 0.6690\n",
      "Iter: 8500, Loss: 0.6653\n",
      "Iter: 9000, Loss: 0.6610\n",
      "Iter: 9500, Loss: 0.6560\n",
      "Iter: 10000, Loss: 0.6501\n",
      "Iter: 10500, Loss: 0.6431\n",
      "Iter: 11000, Loss: 0.6348\n",
      "Iter: 11500, Loss: 0.6249\n",
      "Iter: 12000, Loss: 0.6131\n",
      "Iter: 12500, Loss: 0.5989\n",
      "Iter: 13000, Loss: 0.5817\n",
      "Iter: 13500, Loss: 0.5607\n",
      "Iter: 14000, Loss: 0.5351\n",
      "Iter: 14500, Loss: 0.5039\n",
      "Iter: 15000, Loss: 0.4667\n",
      "Iter: 15500, Loss: 0.4237\n",
      "Iter: 16000, Loss: 0.3762\n",
      "Iter: 16500, Loss: 0.3268\n",
      "Iter: 17000, Loss: 0.2787\n",
      "Iter: 17500, Loss: 0.2345\n",
      "Iter: 18000, Loss: 0.1962\n",
      "Iter: 18500, Loss: 0.1642\n",
      "Iter: 19000, Loss: 0.1380\n",
      "Iter: 19500, Loss: 0.1169\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test 데이터의 값은"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.test_model(x_data, y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
